{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosonai Higgs-Llama-3-70B AWQ 4-bit Quantization\n",
    "\n",
    "This notebook quantizes Bosonai's `Higgs-Llama-3-70B` model using AWQ (Activation-aware Weight Quantization) to 4-bit precision.\n",
    "\n",
    "**Model Info:**\n",
    "- **Model:** bosonai/Higgs-Llama-3-70B\n",
    "- **Size:** 70B parameters (~140GB FP16)\n",
    "- **Released:** August 2024\n",
    "- **Use Case:** Large language model based on Llama 3 architecture\n",
    "- **Status:** No AWQ quantization exists (only GGUF/EXL2)\n",
    "\n",
    "**Memory Requirements:** ~140GB for FP16 loading, requires H200 (141GB) or H100 (80GB with careful management)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip install transformers==4.51.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix AutoAWQ Compatibility\n",
    "\n",
    "AutoAWQ is deprecated and requires transformers 4.51.3 for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade autoawq transformers accelerate datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Memory and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Set memory optimization\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"‚úÖ Memory cleared and optimized\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - AWQ requires CUDA GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"   AutoAWQ version: {__import__('awq').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"bosonai/Higgs-Llama-3-70B\"\n",
    "quant_path = \"higgs-llama-3-70b-awq\"\n",
    "hf_model_id = \"ronantakizawa/higgs-llama-3-70b-awq\"\n",
    "\n",
    "# AWQ quantization config\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}\n",
    "\n",
    "print(f\"üì¶ Model: {model_path}\")\n",
    "print(f\"üíæ Output: {quant_path}\")\n",
    "print(f\"üöÄ Upload to: {hf_model_id}\")\n",
    "print(f\"\\n‚öôÔ∏è  AWQ Config:\")\n",
    "for key, value in quant_config.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"   Total Memory: {total_mem:.1f} GB\")\n",
    "    if total_mem < 140:\n",
    "        print(f\"   ‚ö†Ô∏è  Warning: Model requires ~140GB FP16, you have {total_mem:.1f}GB\")\n",
    "        print(f\"   üí° Consider using H200 (141GB) for safe loading\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No CUDA GPU detected - AWQ requires GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"‚è≥ Loading Higgs-Llama-3-70B...\")\nprint(\"   This is a 70B parameter model (~140GB FP16)\")\nprint(\"   Loading will take 10-20 minutes...\\n\")\n\nstart_time = time.time()\n\ntry:\n    # Load model with safetensors to avoid meta tensor issues\n    model = AutoAWQForCausalLM.from_pretrained(\n        model_path,\n        safetensors=True,  # Avoid meta tensors\n        device_map={\"\": 0},  # Explicit device mapping\n        low_cpu_mem_usage=True,\n        use_cache=False\n    )\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    # Set pad token\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    elapsed = time.time() - start_time\n    \n    print(f\"‚úÖ Model loaded successfully in {elapsed/60:.1f} minutes\")\n    print(f\"   Model type: {type(model).__name__}\")\n    \n    if torch.cuda.is_available():\n        print(f\"   GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        print(f\"   GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Failed to load model: {e}\")\n    print(\"\\nPossible issues:\")\n    print(\"1. Insufficient GPU memory (need ~140GB)\")\n    print(\"2. Use H200 (141GB) or H100 with offloading\")\n    print(\"3. Network/download issues\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Calibration Data\n",
    "\n",
    "For large language models, we use diverse text data for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üìö Loading calibration data from C4...\\n\")\n\n# Load C4 dataset (better than WikiText for general models)\ndataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n\n# Prepare calibration samples\ncalibration_data = []\ntarget_samples = 512\nmin_length = 200\nmax_length = 1000\n\nprint(f\"üîç Filtering criteria:\")\nprint(f\"   ‚Ä¢ Length: {min_length}-{max_length} characters\")\nprint(f\"   ‚Ä¢ Target: {target_samples} samples\\n\")\n\nfor sample in dataset:\n    text = sample.get('text', '').strip()\n    if min_length <= len(text) <= max_length:\n        calibration_data.append(text)\n    if len(calibration_data) >= target_samples:\n        break\n\nprint(f\"‚úÖ Prepared {len(calibration_data)} calibration samples\")\nprint(f\"   Average length: {sum(len(s) for s in calibration_data) // len(calibration_data)} chars\")\n\n# Show token statistics\nsample_tokens = [len(tokenizer.encode(s)) for s in calibration_data[:50]]\nprint(f\"\\nüî¢ Tokenization stats (first 50 samples):\")\nprint(f\"   ‚Ä¢ Token count: min={min(sample_tokens)}, max={max(sample_tokens)}, avg={sum(sample_tokens)//len(sample_tokens)}\")\n\nprint(f\"\\nüìù Sample preview:\")\nprint(f\"   {calibration_data[0][:200]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run AWQ Quantization\n",
    "\n",
    "This will take 1-2 hours for a 70B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîß STARTING AWQ QUANTIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n‚è≥ Quantizing {model_path}...\")\n",
    "print(f\"   Using {len(calibration_data)} calibration samples\")\n",
    "print(f\"   This will take approximately 1-2 hours for 70B model\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Run quantization\n",
    "    model.quantize(\n",
    "        tokenizer,\n",
    "        quant_config=quant_config,\n",
    "        calib_data=calibration_data\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ AWQ quantization completed in {elapsed/60:.1f} minutes!\")\n",
    "    print(f\"   ({elapsed:.0f} seconds)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Quantization failed: {e}\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"1. Out of memory during quantization\")\n",
    "    print(\"2. Model architecture compatibility issues\")\n",
    "    print(\"3. AutoAWQ version needs update\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\nüíæ Saving quantized model to {quant_path}...\\n\")\n\n# Save model and tokenizer\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f\"‚úÖ Model saved successfully!\")\n\n# Check size\ndef get_dir_size(path):\n    total = 0\n    for root, dirs, files in os.walk(path):\n        for f in files:\n            total += os.path.getsize(os.path.join(root, f))\n    return total / (1024**3)\n\nif os.path.exists(quant_path):\n    quantized_size = get_dir_size(quant_path)\n    original_size = 140.0  # ~140GB for 70B FP16\n    \n    print(f\"\\nüìä Size Comparison:\")\n    print(f\"   ‚Ä¢ Original FP16: ~{original_size:.1f} GB\")\n    print(f\"   ‚Ä¢ AWQ 4-bit: {quantized_size:.2f} GB\")\n    print(f\"   ‚Ä¢ Reduction: {((original_size - quantized_size) / original_size * 100):.1f}%\")\n    print(f\"   ‚Ä¢ Compression: {original_size / quantized_size:.1f}x\")\n    \n    # List saved files\n    print(f\"\\nüìÅ Saved files:\")\n    for root, dirs, files in os.walk(quant_path):\n        for file in sorted(files):\n            size = os.path.getsize(os.path.join(root, file)) / (1024**2)\n            print(f\"   ‚Ä¢ {file}: {size:.1f} MB\")\n\n# Clear memory before upload\ndel model\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Create Model Card and Upload to HuggingFace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_card = f\"\"\"---\nlanguage:\n- en\nlicense: llama3\ntags:\n- awq\n- quantized\n- 4-bit\n- llama-3\n- bosonai\nbase_model: bosonai/Higgs-Llama-3-70B\n---\n\n# Higgs-Llama-3-70B AWQ 4-bit Quantized\n\nThis is a 4-bit AWQ quantized version of [bosonai/Higgs-Llama-3-70B](https://huggingface.co/bosonai/Higgs-Llama-3-70B).\n\n## Model Description\n\n- **Base Model:** bosonai/Higgs-Llama-3-70B (70B parameters)\n- **Quantization Method:** AWQ (Activation-aware Weight Quantization)\n- **Quantization Precision:** 4-bit\n- **Group Size:** 128\n- **Original Size:** ~140 GB (FP16)\n- **Quantized Size:** ~35 GB (estimated)\n- **Memory Reduction:** ~75%\n\n## About Higgs-Llama-3-70B\n\nHiggs-Llama-3-70B is a 70B parameter language model based on the Llama 3 architecture, developed by Bosonai.\n\n## Usage\n\n### Using Transformers\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\nimport torch\n\nmodel_id = \"{hf_model_id}\"\n\nquantization_config = AwqConfig(\n    bits=4,\n    fuse_max_seq_len=2048,\n    do_fuse=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n\nprompt = \"Explain the theory of relativity in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n### Using AutoAWQ\n\n```python\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_id = \"{hf_model_id}\"\n\nmodel = AutoAWQForCausalLM.from_quantized(\n    model_id,\n    fuse_layers=True,\n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nprompt = \"Write a Python function to find the longest common subsequence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Installation\n\n```bash\npip install autoawq transformers accelerate\n```\n\n## Requirements\n\n- **GPU Memory:** ~40-45 GB VRAM (runs on A100 80GB, H100, H200)\n- **CUDA:** Required for AWQ\n- **Python:** 3.8+\n\n## Performance\n\n- **Memory Usage:** ~75% reduction vs FP16\n- **Inference Speed:** Fast with AWQ GEMM optimizations\n- **Quality:** Minimal accuracy loss with activation-aware quantization\n- **Use Cases:** Perfect for deploying 70B models on single GPU\n\n## Limitations\n\n- Requires CUDA GPU (no CPU support for AWQ)\n- May have slight quality degradation compared to full precision (~1-3%)\n- Calibration-dependent (quality depends on calibration data)\n- Subject to Llama 3 License terms\n\n## License\n\nLlama 3 License\n\n## Citation\n\n```bibtex\n@misc{{higgs-llama-3-70b-awq,\n  author = {{Ronan Takizawa}},\n  title = {{Higgs-Llama-3-70B AWQ 4-bit Quantized}},\n  year = {{2025}},\n  publisher = {{Hugging Face}},\n  howpublished = {{\\\\url{{https://huggingface.co/{hf_model_id}}}}}\n}}\n```\n\n## Base Model Citation\n\nPlease refer to the [original model card](https://huggingface.co/bosonai/Higgs-Llama-3-70B) for the base model citation.\n\n## Acknowledgments\n\n- Bosonai for the Higgs-Llama-3-70B model\n- MIT HAN Lab for the AWQ quantization method\n- Casper Hansen and the AutoAWQ team\n\"\"\"\n\n# Save model card\nreadme_path = os.path.join(quant_path, \"README.md\")\nwith open(readme_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(model_card)\n\nprint(f\"‚úÖ Model card created at {readme_path}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from huggingface_hub import notebook_login\n\n# Login to Hugging Face\nnotebook_login()\n\nprint(f\"\\nüöÄ Uploading to {hf_model_id}...\\n\")\n\ntry:\n    # Create repository\n    create_repo(hf_model_id, repo_type=\"model\", exist_ok=True)\n    print(f\"‚úÖ Repository ready: {hf_model_id}\")\n    \n    # Upload model files\n    api = HfApi()\n    api.upload_folder(\n        folder_path=quant_path,\n        repo_id=hf_model_id,\n        repo_type=\"model\",\n        commit_message=\"Upload AWQ 4-bit quantized Higgs-Llama-3-70B\"\n    )\n    \n    print(f\"\\n‚úÖ Model successfully uploaded!\")\n    print(f\"   View at: https://huggingface.co/{hf_model_id}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Upload failed: {e}\")\n    print(\"\\nMake sure:\")\n    print(\"1. You're logged in with notebook_login()\")\n    print(\"2. You have write access to the repository\")\n    print(\"3. You have stable internet connection\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Test Loading Quantized Model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"\\nüîÑ Reloading quantized model for testing...\\n\")\n\n# Load quantized model\nmodel_quantized = AutoAWQForCausalLM.from_quantized(\n    quant_path,\n    fuse_layers=True,\n    device_map=\"auto\"\n)\n\nprint(f\"‚úÖ Quantized model loaded successfully\")\n\nif torch.cuda.is_available():\n    mem_allocated = torch.cuda.memory_allocated() / 1024**3\n    print(f\"   GPU Memory: {mem_allocated:.2f} GB\")\n    print(f\"   Memory saved: ~{140.0 - mem_allocated:.1f} GB vs FP16\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Comprehensive Evaluation: Generation Tests & Perplexity\n\nEvaluate model quality and measure perplexity for quantization assessment."
  },
  {
   "cell_type": "code",
   "source": "import math\nimport json\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üß™ COMPREHENSIVE EVALUATION\")\nprint(\"=\"*70)\n\n# Get device for model\ndevice = next(model_quantized.parameters()).device\n\n# Part 1: Generation Quality Tests\nprint(\"\\nüìã Part 1: Generation Quality Tests\\n\")\n\ntest_suite = {\n    \"general_knowledge\": [\n        {\n            \"prompt\": \"Explain the theory of relativity in simple terms.\",\n            \"keywords\": [\"einstein\", \"relativity\", \"space\", \"time\", \"gravity\"]\n        },\n        {\n            \"prompt\": \"What are the main differences between RNA and DNA?\",\n            \"keywords\": [\"rna\", \"dna\", \"nucleotide\", \"uracil\", \"thymine\"]\n        }\n    ],\n    \"reasoning\": [\n        {\n            \"prompt\": \"If it takes 5 machines 5 minutes to make 5 widgets, how long does it take 100 machines to make 100 widgets?\",\n            \"keywords\": [\"5\", \"minutes\", \"same\"]\n        },\n        {\n            \"prompt\": \"Explain the trolley problem and its ethical implications.\",\n            \"keywords\": [\"trolley\", \"ethical\", \"dilemma\", \"utilitarian\", \"choice\"]\n        }\n    ],\n    \"code_generation\": [\n        {\n            \"prompt\": \"Write a Python function to find the longest common subsequence.\",\n            \"keywords\": [\"def\", \"subsequence\", \"return\", \"dynamic\", \"programming\"]\n        }\n    ],\n    \"creative_writing\": [\n        {\n            \"prompt\": \"Write a haiku about artificial intelligence.\",\n            \"keywords\": [\"silicon\", \"mind\", \"algorithm\", \"digital\", \"learn\"]\n        }\n    ]\n}\n\nresults = {}\ntotal_correct = 0\ntotal_tests = 0\ntotal_time = 0\n\nfor category, tests in test_suite.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"üìÇ Category: {category.upper().replace('_', ' ')}\")\n    print('='*70)\n    \n    category_results = []\n    category_correct = 0\n    \n    for i, test in enumerate(tests, 1):\n        prompt = test[\"prompt\"]\n        keywords = [kw.lower() for kw in test[\"keywords\"]]\n        \n        print(f\"\\n{i}. üìù Prompt: {prompt}\")\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        start_time = time.time()\n        outputs = model_quantized.generate(\n            **inputs,\n            max_new_tokens=200,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.95,\n            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id\n        )\n        generation_time = time.time() - start_time\n        \n        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        result_lower = result.lower()\n        \n        # Check keywords\n        found_keywords = [kw for kw in keywords if kw in result_lower]\n        keyword_score = len(found_keywords) / len(keywords)\n        is_correct = keyword_score >= 0.4  # 40% threshold\n        \n        print(f\"   ‚úÖ Output: {result[:200]}{'...' if len(result) > 200 else ''}\")\n        print(f\"   üéØ Keywords found: {len(found_keywords)}/{len(keywords)} ({keyword_score*100:.0f}%)\")\n        print(f\"   {'‚úì' if is_correct else '‚úó'} {'PASS' if is_correct else 'FAIL'}\")\n        print(f\"   ‚è±Ô∏è  Time: {generation_time:.2f}s\")\n        \n        category_results.append({\n            \"prompt\": prompt,\n            \"output\": result,\n            \"keywords_found\": found_keywords,\n            \"keyword_score\": keyword_score,\n            \"pass\": is_correct,\n            \"time\": generation_time\n        })\n        \n        if is_correct:\n            category_correct += 1\n        total_correct += 1 if is_correct else 0\n        total_tests += 1\n        total_time += generation_time\n    \n    results[category] = {\n        \"tests\": category_results,\n        \"accuracy\": category_correct / len(tests),\n        \"avg_time\": sum(t[\"time\"] for t in category_results) / len(tests)\n    }\n    \n    print(f\"\\n{'‚îÄ'*70}\")\n    print(f\"üìä {category.upper().replace('_', ' ')} Summary:\")\n    print(f\"   Accuracy: {category_correct}/{len(tests)} ({results[category]['accuracy']*100:.0f}%)\")\n    print(f\"   Avg Time: {results[category]['avg_time']:.2f}s\")\n\n# Part 2: Perplexity Measurement\nprint(f\"\\n{'='*70}\")\nprint(\"üìê Part 2: Perplexity Measurement\")\nprint('='*70 + \"\\n\")\n\ndef calculate_perplexity(model, tokenizer, texts, max_samples=100):\n    \"\"\"Calculate perplexity on a set of texts\"\"\"\n    device = next(model.parameters()).device\n    model.eval()\n    \n    total_loss = 0\n    total_tokens = 0\n    samples_used = 0\n    \n    print(f\"‚è≥ Calculating perplexity on {min(len(texts), max_samples)} samples...\")\n    \n    with torch.no_grad():\n        for i, text in enumerate(texts[:max_samples]):\n            # Tokenize\n            encodings = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n            input_ids = encodings.input_ids.to(device)\n            \n            # Skip very short sequences\n            if input_ids.shape[1] < 2:\n                continue\n            \n            # Calculate loss\n            outputs = model(input_ids, labels=input_ids)\n            loss = outputs.loss\n            \n            # Accumulate\n            total_loss += loss.item() * input_ids.shape[1]\n            total_tokens += input_ids.shape[1]\n            samples_used += 1\n            \n            if (i + 1) % 25 == 0:\n                print(f\"   Processed {i+1}/{min(len(texts), max_samples)} samples...\")\n    \n    # Calculate perplexity\n    avg_loss = total_loss / total_tokens\n    perplexity = math.exp(avg_loss)\n    \n    return perplexity, avg_loss, samples_used, total_tokens\n\n# Use calibration data for perplexity\nperplexity, avg_loss, samples_used, total_tokens = calculate_perplexity(\n    model_quantized, \n    tokenizer, \n    calibration_data, \n    max_samples=100\n)\n\nprint(f\"\\n‚úÖ Perplexity Calculation Complete:\")\nprint(f\"   ‚Ä¢ Perplexity: {perplexity:.4f}\")\nprint(f\"   ‚Ä¢ Average Loss: {avg_loss:.4f}\")\nprint(f\"   ‚Ä¢ Samples: {samples_used}\")\nprint(f\"   ‚Ä¢ Tokens: {total_tokens:,}\")\nprint(f\"\\n   Interpretation:\")\nif perplexity < 10:\n    print(f\"   üåü EXCELLENT - Very low perplexity (< 10)\")\nelif perplexity < 20:\n    print(f\"   ‚úÖ GOOD - Low perplexity (10-20)\")\nelif perplexity < 40:\n    print(f\"   üëç ACCEPTABLE - Moderate perplexity (20-40)\")\nelse:\n    print(f\"   ‚ö†Ô∏è  HIGH - Consider re-quantization (> 40)\")\n\n# Part 3: Final Summary\nprint(f\"\\n{'='*70}\")\nprint(\"üìä EVALUATION SUMMARY\")\nprint('='*70 + \"\\n\")\n\noverall_accuracy = total_correct / total_tests\navg_latency = total_time / total_tests\n\nprint(f\"üéØ Generation Tests:\")\nprint(f\"   ‚Ä¢ Overall Accuracy: {total_correct}/{total_tests} ({overall_accuracy*100:.0f}%)\")\nprint(f\"   ‚Ä¢ Average Latency: {avg_latency:.2f}s\")\nprint(f\"\\nüìà Per-Category Results:\")\nfor category, result in results.items():\n    print(f\"   ‚Ä¢ {category.replace('_', ' ').title()}: {result['accuracy']*100:.0f}% accuracy, {result['avg_time']:.2f}s avg\")\n\nprint(f\"\\nüìê Perplexity:\")\nprint(f\"   ‚Ä¢ Score: {perplexity:.4f}\")\nprint(f\"   ‚Ä¢ Quality: {'EXCELLENT' if perplexity < 10 else 'GOOD' if perplexity < 20 else 'ACCEPTABLE' if perplexity < 40 else 'HIGH'}\")\n\nif torch.cuda.is_available():\n    mem_allocated = torch.cuda.memory_allocated() / 1024**3\n    print(f\"\\nüíæ GPU Memory:\")\n    print(f\"   ‚Ä¢ Allocated: {mem_allocated:.2f} GB\")\n\n# Save results to JSON\nevaluation_results = {\n    \"model\": model_path,\n    \"quantization\": \"AWQ 4-bit\",\n    \"generation_tests\": {\n        \"overall_accuracy\": overall_accuracy,\n        \"total_correct\": total_correct,\n        \"total_tests\": total_tests,\n        \"avg_latency\": avg_latency,\n        \"by_category\": {\n            cat: {\n                \"accuracy\": res[\"accuracy\"],\n                \"avg_time\": res[\"avg_time\"],\n                \"tests\": len(res[\"tests\"])\n            } for cat, res in results.items()\n        }\n    },\n    \"perplexity\": {\n        \"score\": perplexity,\n        \"avg_loss\": avg_loss,\n        \"samples\": samples_used,\n        \"tokens\": total_tokens\n    },\n    \"gpu_memory_gb\": torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else None\n}\n\nresults_path = os.path.join(quant_path, \"evaluation_results.json\")\nwith open(results_path, \"w\") as f:\n    json.dump(evaluation_results, f, indent=2)\n\nprint(f\"\\nüíæ Results saved to: {results_path}\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ COMPREHENSIVE EVALUATION COMPLETE!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}